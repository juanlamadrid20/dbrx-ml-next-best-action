{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "111b4beb-1886-4f48-86ed-d8dabe307233",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.widgets.removeAll()\n",
    "dbutils.widgets.text(\"catalog\", \"juan_dev\", \"Unity Catalog catalog (required)\")\n",
    "dbutils.widgets.text(\"schema\", \"ml_nba_demo\", \"Schema/database for demo artifacts\")\n",
    "dbutils.widgets.text(\"feature_table\", \"nba_customer_features\", \"Feature table name (no catalog/schema)\")\n",
    "dbutils.widgets.text(\"raw_table\", \"nba_customers_raw\", \"Raw data table name (no catalog/schema)\")\n",
    "dbutils.widgets.text(\"rec_table\", \"nba_recommendations\", \"Recommendation output table name\")\n",
    "dbutils.widgets.text(\"log_table\", \"nba_inference_log\", \"Inference log table name\")\n",
    "dbutils.widgets.text(\"model_base\", \"nba_model\", \"Model base name\")\n",
    "dbutils.widgets.text(\"experiment_path\", \"/Users/juan.lamadrid@databricks.com/experiments/nba_demo\", \"MLflow experiment path (optional)\")\n",
    "\n",
    "CATALOG = dbutils.widgets.get(\"catalog\")\n",
    "SCHEMA = dbutils.widgets.get(\"schema\")\n",
    "FEATURE_TABLE = dbutils.widgets.get(\"feature_table\")\n",
    "RAW_TABLE = dbutils.widgets.get(\"raw_table\")\n",
    "REC_TABLE = dbutils.widgets.get(\"rec_table\")\n",
    "LOG_TABLE = dbutils.widgets.get(\"log_table\")\n",
    "MODEL_BASE = dbutils.widgets.get(\"model_base\")\n",
    "EXPERIMENT_PATH = dbutils.widgets.get(\"experiment_path\") or None\n",
    "\n",
    "FEATURE_TABLE_FULL = f\"{CATALOG}.{SCHEMA}.{FEATURE_TABLE}\"\n",
    "RAW_TABLE_FULL = f\"{CATALOG}.{SCHEMA}.{RAW_TABLE}\"\n",
    "REC_TABLE_FULL = f\"{CATALOG}.{SCHEMA}.{REC_TABLE}\"\n",
    "LOG_TABLE_FULL = f\"{CATALOG}.{SCHEMA}.{LOG_TABLE}\"\n",
    "UC_MODEL_NAME = f\"{CATALOG}.{SCHEMA}.{MODEL_BASE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2478a81-e0e5-4bf5-a518-26f23f898658",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure the MLflow client talks to Unity Catalog's registry (when applicable)\n",
    "import mlflow\n",
    "try:\n",
    "    mlflow.set_registry_uri(\"databricks-uc\")\n",
    "except Exception as e:\n",
    "    print(\"Note: Using default registry URI. If you're on UC, this is usually fine.\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecf26756-db3d-4cc5-b506-e0c93c765432",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "spark.sql(f\"USE {CATALOG}.{SCHEMA}\")\n",
    "\n",
    "print(\"Using configuration:\")\n",
    "print(\"  Catalog       :\", CATALOG)\n",
    "print(\"  Schema        :\", SCHEMA)\n",
    "print(\"  Raw table     :\", RAW_TABLE_FULL)\n",
    "print(\"  Feature table :\", FEATURE_TABLE_FULL)\n",
    "print(\"  Recs table    :\", REC_TABLE_FULL)\n",
    "print(\"  Log table     :\", LOG_TABLE_FULL)\n",
    "print(\"  UC model name :\", UC_MODEL_NAME)\n",
    "print(\"  Experiment    :\", EXPERIMENT_PATH or \"<not set>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f490fee-0906-40a4-82b8-02c886095686",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "n_customers = 10_000\n",
    "customer_id = np.arange(1, n_customers + 1)\n",
    "\n",
    "# Demographics\n",
    "age = np.random.randint(18, 71, size=n_customers)\n",
    "gender = np.random.choice(['M','F'], size=n_customers, p=[0.48,0.52])\n",
    "region = np.random.choice(['Northeast','Midwest','South','West'], size=n_customers, p=[0.20,0.23,0.35,0.22])\n",
    "income = np.random.normal(80_000, 25_000, size=n_customers).clip(20_000, 250_000)\n",
    "\n",
    "# Transactions (last month)\n",
    "num_purchases_last_month = np.random.poisson(lam=2.2, size=n_customers)\n",
    "purchase_amount_last_month = np.random.gamma(shape=2.0, scale=120.0, size=n_customers) * (1 + (income/250_000))\n",
    "\n",
    "# Browsing (last week)\n",
    "browsing_minutes_last_week = np.random.gamma(shape=2.2, scale=25.0, size=n_customers)\n",
    "categories = ['Beauty','Electronics','Fashion','Home','Grocery','Sports','Toys']\n",
    "top_category = np.random.choice(categories, size=n_customers)\n",
    "\n",
    "# Ground-truth NEXT BEST ACTION (rule-based for education)\n",
    "best_action = []\n",
    "for a, spend, purchases, browse in zip(age, purchase_amount_last_month, num_purchases_last_month, browsing_minutes_last_week):\n",
    "    if spend > 600 or purchases >= 6:\n",
    "        best_action.append(\"Email\")\n",
    "    elif browse > 200 or a < 28:\n",
    "        best_action.append(\"Push\")\n",
    "    elif 28 <= a <= 50:\n",
    "        best_action.append(\"SMS\")\n",
    "    else:\n",
    "        best_action.append(\"Email\")\n",
    "\n",
    "raw_pdf = pd.DataFrame({\n",
    "    \"customer_id\": customer_id,\n",
    "    \"age\": age,\n",
    "    \"gender\": gender,\n",
    "    \"region\": region,\n",
    "    \"income\": income.round(2),\n",
    "    \"num_purchases_last_month\": num_purchases_last_month,\n",
    "    \"purchase_amount_last_month\": purchase_amount_last_month.round(2),\n",
    "    \"browsing_minutes_last_week\": browsing_minutes_last_week.round(2),\n",
    "    \"top_category\": top_category,\n",
    "    \"best_action\": best_action\n",
    "})\n",
    "\n",
    "raw_sdf = spark.createDataFrame(raw_pdf)\n",
    "raw_sdf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(RAW_TABLE_FULL)\n",
    "\n",
    "display(spark.table(RAW_TABLE_FULL).limit(10))\n",
    "print(f\"Wrote {RAW_TABLE_FULL} with {raw_sdf.count():,} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41bc0403-d0d6-4b13-b72d-f7d4f54595a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "raw = spark.table(RAW_TABLE_FULL)\n",
    "\n",
    "# Class balance\n",
    "display(raw.groupBy(\"best_action\").count().orderBy(F.desc(\"count\")))\n",
    "\n",
    "# Numeric summary\n",
    "num_cols = [\"age\",\"income\",\"num_purchases_last_month\",\"purchase_amount_last_month\",\"browsing_minutes_last_week\"]\n",
    "display(raw.select(num_cols).summary())\n",
    "\n",
    "# Correlation (numeric features vs a numeric label proxy)\n",
    "label_int = F.when(F.col(\"best_action\")==\"Email\", 0).when(F.col(\"best_action\")==\"SMS\", 1).otherwise(2)\n",
    "corr_df = raw.select(*num_cols, label_int.alias(\"label_int\"))\n",
    "\n",
    "corrs = []\n",
    "for c in num_cols:\n",
    "    cor = corr_df.stat.corr(c, \"label_int\")\n",
    "    corrs.append((c, cor))\n",
    "spark.createDataFrame(corrs, [\"feature\",\"corr_with_label\"]).createOrReplaceTempView(\"feat_corrs\")\n",
    "display(spark.sql(\"SELECT * FROM feat_corrs ORDER BY ABS(corr_with_label) DESC\"))\n",
    "\n",
    "# Small sample to pandas for a couple of simple plots (matplotlib only)\n",
    "sample_pdf = raw.limit(5000).toPandas()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(sample_pdf[\"age\"], bins=25)\n",
    "plt.title(\"Age Distribution\"); plt.xlabel(\"Age\"); plt.ylabel(\"Count\")\n",
    "display(plt.gcf()); plt.close()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "groups = [sample_pdf[sample_pdf[\"best_action\"]==lab][\"browsing_minutes_last_week\"] for lab in [\"Email\",\"SMS\",\"Push\"]]\n",
    "plt.boxplot(groups, labels=[\"Email\",\"SMS\",\"Push\"])\n",
    "plt.title(\"Browsing Minutes vs. Best Action\"); plt.xlabel(\"Best Action\"); plt.ylabel(\"Browsing Minutes (last week)\")\n",
    "display(plt.gcf()); plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e1bfc21-41d5-4057-b940-42bf5f35729e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "raw = spark.table(RAW_TABLE_FULL)\n",
    "\n",
    "features_sdf = (\n",
    "    raw\n",
    "    .withColumn(\"gender_ix\", F.when(F.col(\"gender\")==\"M\", 1).otherwise(0))\n",
    "    .withColumn(\"age_bucket\",\n",
    "        F.when(F.col(\"age\")<28, \"young\")\n",
    "         .when((F.col(\"age\")>=28) & (F.col(\"age\")<=50), \"mid\")\n",
    "         .otherwise(\"senior\"))\n",
    "    .withColumn(\"avg_purchase_value\",\n",
    "        F.when(F.col(\"num_purchases_last_month\")>0, F.col(\"purchase_amount_last_month\")/F.col(\"num_purchases_last_month\")).otherwise(0.0))\n",
    "    .withColumn(\"is_high_spender\", (F.col(\"purchase_amount_last_month\")>600).cast(\"int\"))\n",
    "    .select(\n",
    "        \"customer_id\",\n",
    "        \"age\",\"gender_ix\",\"income\",\n",
    "        \"num_purchases_last_month\",\"purchase_amount_last_month\",\"avg_purchase_value\",\n",
    "        \"browsing_minutes_last_week\",\"is_high_spender\",\n",
    "        \"region\",\"top_category\",\"age_bucket\"\n",
    "    )\n",
    ")\n",
    "\n",
    "labels_sdf = raw.select(\n",
    "    \"customer_id\",\n",
    "    F.when(F.col(\"best_action\")==\"Email\", 0).when(F.col(\"best_action\")==\"SMS\", 1).otherwise(2).alias(\"action_label\"),\n",
    "    \"best_action\"\n",
    ")\n",
    "\n",
    "# Persist engineered features to Delta (Feature Store will build on top of this)\n",
    "features_sdf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(FEATURE_TABLE_FULL)\n",
    "display(spark.table(FEATURE_TABLE_FULL).limit(10))\n",
    "display(labels_sdf.limit(10))\n",
    "\n",
    "# We'll set fs_mode now; Cell 3.2 may update it to 'feature_store'\n",
    "fs_mode = \"delta\"\n",
    "print(\"Baseline features table written:\", FEATURE_TABLE_FULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c79fa9b1-4b03-4557-84b7-ef1824b5cebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure the engineered features table has a valid PK column for Feature Store\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "feat_df = spark.table(FEATURE_TABLE_FULL)\n",
    "\n",
    "dups = (feat_df.groupBy(\"customer_id\").count().filter(\"count > 1\").count())\n",
    "nulls = feat_df.filter(F.col(\"customer_id\").isNull()).count()\n",
    "\n",
    "print(f\"Duplicates on customer_id: {dups}\")\n",
    "print(f\"Null customer_id values : {nulls}\")\n",
    "\n",
    "# If needed, auto-deduplicate by customer_id (keep the first) and drop nulls\n",
    "if dups > 0 or nulls > 0:\n",
    "    print(\"Fixing duplicates/nulls by dropping duplicates and null customer_id rows...\")\n",
    "    fixed = (\n",
    "        feat_df\n",
    "        .filter(F.col(\"customer_id\").isNotNull())\n",
    "        .dropDuplicates([\"customer_id\"])\n",
    "    )\n",
    "    fixed.write.mode(\"overwrite\").format(\"delta\").saveAsTable(FEATURE_TABLE_FULL)\n",
    "    feat_df = spark.table(FEATURE_TABLE_FULL)\n",
    "    print(\"Rewrote features table without duplicates/nulls.\")\n",
    "\n",
    "# Enforce NOT NULL on customer_id\n",
    "try:\n",
    "    spark.sql(f\"ALTER TABLE {FEATURE_TABLE_FULL} ALTER COLUMN customer_id SET NOT NULL\")\n",
    "    print(\"Set NOT NULL on customer_id.\")\n",
    "except Exception as e:\n",
    "    print(\"Note: Could not set NOT NULL (might already be set):\", str(e))\n",
    "\n",
    "# Add PRIMARY KEY constraint (constraint name cannot contain dots)\n",
    "CONSTRAINT_NAME = f\"{FEATURE_TABLE}_pk\"\n",
    "try:\n",
    "    spark.sql(f\"ALTER TABLE {FEATURE_TABLE_FULL} ADD CONSTRAINT {CONSTRAINT_NAME} PRIMARY KEY (customer_id)\")\n",
    "    print(f\"Added PRIMARY KEY constraint {CONSTRAINT_NAME} on {FEATURE_TABLE_FULL}.\")\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e).lower():\n",
    "        print(f\"PRIMARY KEY constraint {CONSTRAINT_NAME} already exists; continuing.\")\n",
    "    else:\n",
    "        print(\"PK add failed (check privileges / UC support). Proceeding with Delta-only if FS unavailable.\")\n",
    "        print(\"Details:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b97b2a0-d308-4fa4-8976-6c71608a547a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Try to register/write via Feature Store; if not available, stay in Delta-only mode.\n",
    "fs_mode = \"delta\"\n",
    "try:\n",
    "    from databricks.feature_store import FeatureStoreClient\n",
    "    fs = FeatureStoreClient()\n",
    "\n",
    "    # (Re)create FS table entry if needed\n",
    "    try:\n",
    "        fs.create_table(\n",
    "            name=FEATURE_TABLE_FULL,\n",
    "            primary_keys=[\"customer_id\"],\n",
    "            schema=spark.table(FEATURE_TABLE_FULL).schema,\n",
    "            description=\"Customer features for NBA model (demo)\"\n",
    "        )\n",
    "        print(\"Feature Store table entry created.\")\n",
    "    except Exception as e:\n",
    "        print(\"Feature Store table may already exist; proceeding. Details:\", str(e))\n",
    "\n",
    "    # Write features into FS-managed table\n",
    "    fs.write_table(\n",
    "        name=FEATURE_TABLE_FULL,\n",
    "        df=spark.table(FEATURE_TABLE_FULL),\n",
    "        mode=\"overwrite\"\n",
    "    )\n",
    "    fs_mode = \"feature_store\"\n",
    "    print(f\"Feature Store write OK → {FEATURE_TABLE_FULL}\")\n",
    "except Exception as e:\n",
    "    print(\"Feature Store not available; continuing with Delta-only features.\")\n",
    "    print(\"Reason:\", str(e))\n",
    "\n",
    "print(\"Feature persistence mode:\", fs_mode)\n",
    "display(spark.table(FEATURE_TABLE_FULL).limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddf7d4d8-51cc-4cd9-974f-cea52b65faae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the feature lists (used consistently across train & inference)\n",
    "NUMERIC_FEATURES = [\n",
    "    \"age\",\"gender_ix\",\"income\",\n",
    "    \"num_purchases_last_month\",\"purchase_amount_last_month\",\"avg_purchase_value\",\n",
    "    \"browsing_minutes_last_week\",\"is_high_spender\"\n",
    "]\n",
    "CATEGORICALS = [\"region\",\"top_category\",\"age_bucket\"]\n",
    "\n",
    "# Assemble training dataframe (Spark → Pandas)\n",
    "# if fs_mode == \"feature_store\":\n",
    "from databricks.feature_store import FeatureLookup\n",
    "feature_lookups = [\n",
    "    FeatureLookup(\n",
    "        table_name=FEATURE_TABLE_FULL,\n",
    "        feature_names=NUMERIC_FEATURES + CATEGORICALS,\n",
    "        lookup_key=\"customer_id\"\n",
    "    )\n",
    "]\n",
    "training_set = fs.create_training_set(df=labels_sdf, feature_lookups=feature_lookups, label=\"action_label\")\n",
    "train_sdf = training_set.load_df()\n",
    "# else:\n",
    "#     feats = spark.table(FEATURE_TABLE_FULL)\n",
    "#     train_sdf = labels_sdf.alias(\"l\").join(feats.alias(\"f\"), on=\"customer_id\") \\\n",
    "#         .select(\"l.action_label\", *NUMERIC_FEATURES, *CATEGORICALS, \"f.customer_id\")\n",
    "\n",
    "train_pdf = train_sdf.toPandas()\n",
    "\n",
    "# Build design matrix (one-hot categoricals)\n",
    "import pandas as pd\n",
    "y_all = train_pdf[\"action_label\"].astype(int).values\n",
    "X_all = train_pdf[NUMERIC_FEATURES + CATEGORICALS].copy()\n",
    "X_all = pd.get_dummies(X_all, columns=CATEGORICALS, drop_first=True)\n",
    "\n",
    "# Persist training column order for downstream inference alignment\n",
    "TRAIN_COLUMNS = list(X_all.columns)\n",
    "print(\"Training matrix shape:\", X_all.shape)\n",
    "print(\"Label shape:\", y_all.shape)\n",
    "print(\"First 5 training columns:\", TRAIN_COLUMNS[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b248f87d-9bdd-4a7d-bd42-899e6c8ec986",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow, mlflow.sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np, matplotlib.pyplot as plt, itertools\n",
    "\n",
    "EXPERIMENT_PATH = f\"/Users/juan.lamadrid@databricks.com/experiments/nba-model\"\n",
    "# (Optional) Set experiment explicitly\n",
    "if EXPERIMENT_PATH:\n",
    "    mlflow.set_experiment(EXPERIMENT_PATH)\n",
    "\n",
    "# Ensure UC registry when available (safe if already set)\n",
    "try:\n",
    "    mlflow.set_registry_uri(\"databricks-uc\")\n",
    "    print(\"MLflow registry set to Unity Catalog.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Unity Catalog MLflow registry is required for this workflow.\") from e\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=0.25, random_state=42, stratify=y_all\n",
    ")\n",
    "\n",
    "mlflow.sklearn.autolog(log_input_examples=True, log_model_signatures=True)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    model = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Basic eval\n",
    "    y_pred = model.predict(X_test)\n",
    "    TEST_ACCURACY = float(accuracy_score(y_test, y_pred))\n",
    "    mlflow.log_metric(\"test_accuracy\", TEST_ACCURACY)\n",
    "    mlflow.log_text(classification_report(y_test, y_pred), \"classification_report.txt\")\n",
    "\n",
    "    # Confusion matrix artifact\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=[0,1,2])\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(cm, interpolation='nearest'); plt.title(\"Confusion Matrix (0=Email,1=SMS,2=Push)\")\n",
    "    plt.colorbar()\n",
    "    ticks = np.arange(3); labs = [\"Email\",\"SMS\",\"Push\"]\n",
    "    plt.xticks(ticks, labs, rotation=45); plt.yticks(ticks, labs)\n",
    "    thresh = cm.max()/2.0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, f\"{cm[i,j]:d}\", ha=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel(\"True\"); plt.xlabel(\"Pred\"); plt.tight_layout()\n",
    "    mlflow.log_figure(plt.gcf(), \"confusion_matrix.png\"); plt.close()\n",
    "\n",
    "    # ✅ Log model using the modern parameter `name`, not deprecated `artifact_path`\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=model,\n",
    "        name=\"model\",\n",
    "        signature=mlflow.models.infer_signature(X_train, model.predict(X_train))\n",
    "    )\n",
    "\n",
    "    RUN_ID = run.info.run_id\n",
    "    MODEL_RUNS_URI = f\"runs:/{RUN_ID}/model\"   # <-- use this for registration (works with UC)\n",
    "\n",
    "print(\"Run ID:\", RUN_ID)\n",
    "print(\"Model runs URI:\", MODEL_RUNS_URI)\n",
    "print(\"Test accuracy:\", TEST_ACCURACY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48201e3e-3a35-47e4-bae6-860f1401aa6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "new_version = None\n",
    "\n",
    "try:\n",
    "    # ✅ UC registration: use runs:/<run_id>/model\n",
    "    mv = mlflow.register_model(model_uri=MODEL_RUNS_URI, name=UC_MODEL_NAME)\n",
    "    new_version = int(mv.version)\n",
    "    print(f\"Registered to UC: {UC_MODEL_NAME} v{new_version}\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Unity Catalog model registration failed. Verify catalog/schema permissions and UC configuration.\") from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d94a1bea-345e-479e-bb64-3da56f979c23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "import mlflow\n",
    "\n",
    "client = MlflowClient()\n",
    "model_name = UC_MODEL_NAME\n",
    "if new_version is None:\n",
    "    raise RuntimeError(\"Model registration did not return a version. Re-run the registration cell.\")\n",
    "new_v = int(new_version)\n",
    "\n",
    "def get_alias_version_or_none(name, alias):\n",
    "    try:\n",
    "        mv = client.get_model_version_by_alias(name, alias)\n",
    "        return int(mv.version)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def get_metric_for_version(name, version, metric_key=\"test_accuracy\"):\n",
    "    if version is None:\n",
    "        return None\n",
    "    mv = client.get_model_version(name, str(version))\n",
    "    r = mlflow.get_run(mv.run_id)\n",
    "    return r.data.metrics.get(metric_key)\n",
    "\n",
    "# Always mark latest as @staging for smoke tests / endpoints\n",
    "try:\n",
    "    client.set_registered_model_alias(model_name, \"staging\", str(new_v))\n",
    "    print(f\"Set alias @{model_name}@staging → v{new_v}\")\n",
    "except Exception as e:\n",
    "    print(\"Alias set (staging) warning:\", str(e))\n",
    "\n",
    "# Champion vs Challenger decision by test_accuracy\n",
    "champ_v = get_alias_version_or_none(model_name, \"champion\")\n",
    "new_acc = get_metric_for_version(model_name, new_v) or float(\"-inf\")\n",
    "champ_acc = get_metric_for_version(model_name, champ_v) if champ_v else None\n",
    "\n",
    "if champ_v is None:\n",
    "    client.set_registered_model_alias(model_name, \"champion\", str(new_v))\n",
    "    print(f\"No champion existed. Set @{model_name}@champion → v{new_v}\")\n",
    "else:\n",
    "    if new_acc >= (champ_acc or float(\"-inf\")):\n",
    "        client.set_registered_model_alias(model_name, \"prev_champion\", str(champ_v))\n",
    "        client.set_registered_model_alias(model_name, \"champion\", str(new_v))\n",
    "        print(f\"Promoted new champion: v{new_v} (prev champion was v{champ_v})\")\n",
    "    else:\n",
    "        client.set_registered_model_alias(model_name, \"challenger\", str(new_v))\n",
    "        print(f\"Kept champion v{champ_v}; set challenger → v{new_v} (new_acc={new_acc:.4f} < champ_acc={champ_acc:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0f0a419-f97e-41bd-991e-060f62c1b9b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "import pandas as pd\n",
    "\n",
    "# Take a fresh slice of features to score\n",
    "features_df = spark.table(FEATURE_TABLE_FULL).limit(5000).toPandas()\n",
    "\n",
    "# Build inference matrix matching training columns\n",
    "X_infer = features_df[\n",
    "    [\"age\",\"gender_ix\",\"income\",\n",
    "     \"num_purchases_last_month\",\"purchase_amount_last_month\",\"avg_purchase_value\",\n",
    "     \"browsing_minutes_last_week\",\"is_high_spender\",\n",
    "     \"region\",\"top_category\",\"age_bucket\"]\n",
    "].copy()\n",
    "\n",
    "X_infer = pd.get_dummies(X_infer, columns=[\"region\",\"top_category\",\"age_bucket\"], drop_first=True)\n",
    "\n",
    "# Align to training design matrix columns\n",
    "missing = set(TRAIN_COLUMNS) - set(X_infer.columns)\n",
    "for c in missing:\n",
    "    X_infer[c] = 0\n",
    "X_infer = X_infer[TRAIN_COLUMNS]  # exact order\n",
    "\n",
    "# Load model by alias (works for UC & workspace registries)\n",
    "loaded_model = mlflow.pyfunc.load_model(f\"models:/{model_name}@champion\")\n",
    "\n",
    "pred_int = loaded_model.predict(X_infer)\n",
    "inv_map = {0:\"Email\",1:\"SMS\",2:\"Push\"}\n",
    "pred_str = [inv_map[int(i)] for i in pred_int]\n",
    "\n",
    "recs_pdf = pd.DataFrame({\n",
    "    \"customer_id\": features_df[\"customer_id\"].values,\n",
    "    \"recommended_action\": pred_str,\n",
    "    \"scored_at_ts\": pd.Timestamp.utcnow()\n",
    "})\n",
    "recs_sdf = spark.createDataFrame(recs_pdf)\n",
    "recs_sdf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(REC_TABLE_FULL)\n",
    "\n",
    "display(spark.table(REC_TABLE_FULL).limit(10))\n",
    "print(f\"Wrote recommendations → {REC_TABLE_FULL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb1f4115-1361-48a0-8725-12370cf917eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Build an inference log joining predictions + features (simulate a daily batch)\n",
    "preds = spark.table(REC_TABLE_FULL)\n",
    "feats = spark.table(FEATURE_TABLE_FULL).select(\n",
    "    \"customer_id\",\n",
    "    \"age\",\"gender_ix\",\"income\",\n",
    "    \"num_purchases_last_month\",\"purchase_amount_last_month\",\"avg_purchase_value\",\n",
    "    \"browsing_minutes_last_week\",\"is_high_spender\",\n",
    "    \"region\",\"top_category\",\"age_bucket\"\n",
    ")\n",
    "\n",
    "log_df = (\n",
    "    preds.alias(\"p\")\n",
    "    .join(feats.alias(\"f\"), on=\"customer_id\", how=\"left\")\n",
    "    .withColumn(\"log_date\", F.current_date())\n",
    ")\n",
    "\n",
    "(log_df.write.mode(\"append\").format(\"delta\").saveAsTable(LOG_TABLE_FULL))\n",
    "\n",
    "display(spark.table(LOG_TABLE_FULL).orderBy(F.desc(\"scored_at_ts\")).limit(10))\n",
    "\n",
    "# 1) Action distribution over time\n",
    "action_dist = (\n",
    "    spark.table(LOG_TABLE_FULL)\n",
    "    .groupBy(\"log_date\", \"recommended_action\")\n",
    "    .count()\n",
    "    .orderBy(\"log_date\", \"recommended_action\")\n",
    ")\n",
    "display(action_dist)\n",
    "\n",
    "# 2) Simple mean-drift check vs earliest day for numeric features\n",
    "NUMERIC_FEATURES = [\n",
    "    \"age\",\"gender_ix\",\"income\",\n",
    "    \"num_purchases_last_month\",\"purchase_amount_last_month\",\"avg_purchase_value\",\n",
    "    \"browsing_minutes_last_week\",\"is_high_spender\"\n",
    "]\n",
    "log_tbl = spark.table(LOG_TABLE_FULL)\n",
    "dates = [r[\"log_date\"] for r in log_tbl.select(\"log_date\").distinct().orderBy(\"log_date\").collect()]\n",
    "if len(dates) >= 1:\n",
    "    baseline = dates[0]\n",
    "    cur = dates[-1]\n",
    "    base_means = (log_tbl.filter(F.col(\"log_date\")==baseline)\n",
    "                  .select([F.mean(c).alias(c) for c in NUMERIC_FEATURES])\n",
    "                  .collect()[0].asDict())\n",
    "    cur_means = (log_tbl.filter(F.col(\"log_date\")==cur)\n",
    "                 .select([F.mean(c).alias(c) for c in NUMERIC_FEATURES])\n",
    "                 .collect()[0].asDict())\n",
    "    rows = []\n",
    "    for c in NUMERIC_FEATURES:\n",
    "        b = float(base_means.get(c, 0) or 0)\n",
    "        t = float(cur_means.get(c, 0) or 0)\n",
    "        rows.append((c, b, t, (t-b)))\n",
    "    spark.createDataFrame(rows, [\"feature\",\"baseline_mean\",\"current_mean\",\"delta\"]).createOrReplaceTempView(\"drift_means\")\n",
    "    display(spark.sql(\"SELECT * FROM drift_means ORDER BY ABS(delta) DESC\"))\n",
    "else:\n",
    "    print(\"Not enough days logged yet to compute drift deltas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbd23196-208d-4dde-af81-ca906bf9871a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# In the Databricks UI: Serving → Create Endpoint → Select your registered model by alias (@champion)\n",
    "# Then call it like below (fill in your workspace URL, PAT, and endpoint name).\n",
    "# Many teams instead just load by alias in scheduled jobs.\n",
    "\n",
    "import pandas as pd, json, requests\n",
    "\n",
    "DATABRICKS_HOST = \"https://<your-workspace-url>\"  # e.g., https://abc-123.cloud.databricks.com\n",
    "TOKEN = \"<DATABRICKS_PAT>\"\n",
    "ENDPOINT_NAME = \"<your-endpoint-name>\"\n",
    "\n",
    "# Build a single-row payload that matches the training columns\n",
    "features_one = spark.table(FEATURE_TABLE_FULL).limit(1).toPandas()\n",
    "\n",
    "X_row = features_one[\n",
    "    [\"age\",\"gender_ix\",\"income\",\n",
    "     \"num_purchases_last_month\",\"purchase_amount_last_month\",\"avg_purchase_value\",\n",
    "     \"browsing_minutes_last_week\",\"is_high_spender\",\n",
    "     \"region\",\"top_category\",\"age_bucket\"]\n",
    "].copy()\n",
    "X_row = pd.get_dummies(X_row, columns=[\"region\",\"top_category\",\"age_bucket\"], drop_first=True)\n",
    "\n",
    "missing = set(TRAIN_COLUMNS) - set(X_row.columns)\n",
    "for c in missing:\n",
    "    X_row[c] = 0\n",
    "X_row = X_row[TRAIN_COLUMNS]\n",
    "\n",
    "payload = {\"dataframe_records\": X_row.to_dict(orient=\"records\")}\n",
    "url = f\"{DATABRICKS_HOST}/serving-endpoints/{ENDPOINT_NAME}/invocations\"\n",
    "headers = {\"Authorization\": f\"Bearer {TOKEN}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "print(\"Fill in DATABRICKS_HOST, TOKEN, ENDPOINT_NAME and uncomment the POST to invoke.\")\n",
    "# resp = requests.post(url, headers=headers, data=json.dumps(payload), timeout=15)\n",
    "# print(resp.status_code, resp.text)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_65bc13ea-276c-4905-a728-9fe2fb1780e2",
    "environment_version": "3"
   },
   "inputWidgetPreferences": {
    "autoRunOnWidgetChange": "auto-run-selected-command"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "next-best-action",
   "widgets": {
    "catalog": {
     "currentValue": "juan_dev",
     "nuid": "2a502155-97ba-4eac-af22-12f6d259606f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "juan_dev",
      "label": "Unity Catalog catalog (required)",
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "juan_dev",
      "label": "Unity Catalog catalog (required)",
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "experiment_path": {
     "currentValue": "/Users/juan.lamadrid@databricks.com/experiments/nba_demo",
     "nuid": "c344b501-2f8b-4ccb-a104-1728fa22726f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Users/juan.lamadrid@databricks.com/experiments/nba_demo",
      "label": "MLflow experiment path (optional)",
      "name": "experiment_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/Users/juan.lamadrid@databricks.com/experiments/nba_demo",
      "label": "MLflow experiment path (optional)",
      "name": "experiment_path",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "feature_table": {
     "currentValue": "nba_customer_features",
     "nuid": "11ad1182-ced4-497a-922c-c8068f50deac",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "nba_customer_features",
      "label": "Feature table name (no catalog/schema)",
      "name": "feature_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "nba_customer_features",
      "label": "Feature table name (no catalog/schema)",
      "name": "feature_table",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "log_table": {
     "currentValue": "nba_inference_log",
     "nuid": "fe310971-ed54-47c5-81ce-5e622332e46a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "nba_inference_log",
      "label": "Inference log table name",
      "name": "log_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "nba_inference_log",
      "label": "Inference log table name",
      "name": "log_table",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "model_base": {
     "currentValue": "nba_model",
     "nuid": "9851dd35-2263-4ad7-a2b8-427f5162a1dc",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "nba_model",
      "label": "Model base name",
      "name": "model_base",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "nba_model",
      "label": "Model base name",
      "name": "model_base",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "raw_table": {
     "currentValue": "nba_customers_raw",
     "nuid": "7711b328-7d15-416b-93f0-d56c8a33c638",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "nba_customers_raw",
      "label": "Raw data table name (no catalog/schema)",
      "name": "raw_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "nba_customers_raw",
      "label": "Raw data table name (no catalog/schema)",
      "name": "raw_table",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "rec_table": {
     "currentValue": "nba_recommendations",
     "nuid": "82e8dbff-9b91-4631-aade-6730ad82732a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "nba_recommendations",
      "label": "Recommendation output table name",
      "name": "rec_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "nba_recommendations",
      "label": "Recommendation output table name",
      "name": "rec_table",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "schema": {
     "currentValue": "ml_nba_demo",
     "nuid": "45730480-6005-43b8-8e03-82699b16f1af",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "ml_nba_demo",
      "label": "Schema/database for demo artifacts",
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "ml_nba_demo",
      "label": "Schema/database for demo artifacts",
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
